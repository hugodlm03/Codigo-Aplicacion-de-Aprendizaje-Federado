# Aprendizaje Federado con XGBoost y Flower

> **Proyecto final de TFG â€“ Ejemplo completo y adaptable**

Este repositorio muestra cÃ³mo entrenar modelos **XGBoost** de forma **federada** (simulada) con la ayuda de
[**Flower**](https://flower.ai). El objetivo es ofrecer un _framework_ ligero pero extensible que permita

* crear cientos de configuraciones de experimento mediante archivos **TOML**;
* lanzar esos experimentos de manera interactiva o en lote;
* recopilar y comparar mÃ©tricas tanto del **servidor** como de los **clientes**;
* probar estrategias de agregaciÃ³n como **bagging** o **cyclic boosting**;
* mantener los datos *en origen*, preservando la privacidad de cada particiÃ³n.


## Requisitos
* **Python â‰¥â€¯3.10**
* Sistema operativo Windows, macOS o Linux
* ConexiÃ³n a Internet para descargar dependencias y datasets pÃºblicos

| Paquete       | VersiÃ³n mÃ­nima |
|---------------|----------------|
| `flwr`        | 1.18.0         |
| `xgboost`     | 2.0.0          |
| `flwr-datasets` | 0.5.0       |
| `pandas`      | 2.3.0          |
| `questionary` | 2.1.0          |

> **Tip**: todas las dependencias se resuelven automÃ¡ticamente al instalar el proyecto en modo editable; consulta la secciÃ³n siguiente.

---

## InstalaciÃ³n
```bash
# 1) Clona el repositorio y entra en Ã©l
$ git clone <url-del-repo>
$ cd Codigo-Aplicacion-de-Aprendizaje-Federado

# 2) Crea y activa un entorno virtual (opcional pero recomendado)
$ python -m venv .venv
$ source .venv/bin/activate      # Linux/Mac
# .\.venv\Scripts\Activate      # Windows PowerShell

# 3) Instala el proyecto en modo editable (incluye dependencias)
$ pip install -e .
```

Si prefieres un enfoque tradicional, instala desde `requirements.txt`:
```bash
$ pip install -r requirements.txt
```

---

## Estructura del repositorio
```
.
â”œâ”€â”€ configs/                 # Archivos TOML (autogenerados) con combinaciones de parÃ¡metros
â”œâ”€â”€ results/                 # CSV de mÃ©tricas producidos tras los experimentos
â”œâ”€â”€ scripts/             # CLI: gen_configs, run_experiments, inspect_flwr_config
â”œâ”€â”€xgboost_comprehensive/
â”‚Â Â      â”œâ”€â”€ client_app.py    # LÃ³gica del cliente Flower
â”‚Â Â      â”œâ”€â”€ server_app.py    # LÃ³gica del servidor Flower
â”‚Â Â      â”œâ”€â”€ data_loader.py   # Limpieza y particionado del dataset Adidas
â”‚Â Â      â”œâ”€â”€ task.py          # Particionado basado en HuggingFace (dataset Higgs)
â”‚Â Â      â”œâ”€â”€ task_adidas.py   # Particionado basado en Adidas (por regiÃ³n, retailer, ciudad)
â”‚Â Â      â””â”€â”€ â€¦
â””â”€â”€ README.md                # Este documento
â””â”€â”€ requirements.txt
```

---

## Flujo de trabajo rÃ¡pido

### 1. Generar configuraciones
Crea todos los archivos **TOML** necesarios para tus experimentos:
```bash
$ python src/scripts/gen_configs.py
```
Por defecto se exploran distintos valores de _partitioner_, _strategy_, _testâ€‘fraction_, etc.

### 2. Lanzar experimentos
Ejecuta **uno** o **todos** los experimentos de la carpeta `configs/` con un sencillo asistente interactivo:
```bash
$ python src/scripts/run_experiments.py
```
Cada ejecuciÃ³n guarda las mÃ©tricas globales en `results/<nombreâ€‘config>.csv`.

### 3. Analizar resultados
Esto no he conseguido que me funcione, pero es la idea.

Abre los CSV de `results/` con tu herramienta favorita (Excel, pandas, Tableauâ€¦).
Cada fila incluye:
* `round`  Â â€” nÃºmero de ronda
* `rmse`   Â â€” mÃ©trica de validaciÃ³n global o local
* `train_time` â€” duraciÃ³n total del experimento

---





## Detalles del dataset
* **Adidasâ€¯USâ€¯Sales** (`datos/Adidas US Sales Datasets.xlsx`)
  * Limpieza y tipado automÃ¡tico vÃ­a `data_loader.py`.
  * Particionamiento configurable:
    * `region` â€” 5 nodos (Oeste, Este, Centroâ€¦)
    * `retailer_region` â€” 28 nodos
    * `retailer_city` â€” 108 nodos

> Cada particiÃ³n se transforma a `xgb.DMatrix`, manteniendo los datos inâ€‘situ.

---

## Archivos de configuraciÃ³n
Los archivos **TOML** de `configs/` fusionan parÃ¡metros de Flower y de XGBoost.
Un ejemplo mÃ­nimo:
```toml
run-id          = "demo"
strategy        = "bagging"         # o "cycling"
partitioner     = "retailer_region"
centralised-eval = true              # eval serverâ€‘side
local-epochs    = 5                 # Ã¡rboles entrenados por cliente y ronda

[params]
eta         = 0.1
max_depth   = 8
subsample   = 1.0
```
Modifica cualquier clave y vuelve a ejecutar el experimento.

---

## CrÃ©ditos y licenciamiento
Basado en el ejemplo oficial _Comprehensive XGBoostÂ + Flower_ Â©â€¯Flowerâ€¯Labs. CÃ³digo bajo licencia **ApacheÂ 2.0**.

```
@article{beutel2020flower,
  title     = {Flower: A Friendly Federated Learning Research Framework},
  author    = {Beutel, Daniel J and Topal, Taner and Mathur, Akhil and â€¦},
  journal   = {arXiv preprint arXiv:2007.14390},
  year      = {2020}
}
```

Â¡Felices experimentos federados! ðŸš€

